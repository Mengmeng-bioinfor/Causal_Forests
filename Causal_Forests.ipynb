{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156d3252-11f5-40ea-b9cb-af8c35fb3964",
   "metadata": {},
   "source": [
    "# 🎯 Causal Forest Preprocessing for Customer Screen Preference Analysis\n",
    "\n",
    "This script performs data preprocessing to prepare for causal inference using the **Causal Forest DML** method from the `econml` library. The goal is to estimate the causal effect of screen type (Large vs. Small) on user \"Level\" performance using various customer features.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Dataset Overview\n",
    "\n",
    "- 📄 File: `RSR Largescreen_Smallscreen customer.xlsx`\n",
    "- 🔍 Features:\n",
    "  - `Feature T1`, `Feature M1`, `Feature C1`: Numeric covariates\n",
    "  - `Screen`: Treatment variable (categorical: \"L\" = large screen)\n",
    "  - `Level`: Outcome variable\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Processing Steps\n",
    "\n",
    "### 1. Load and Clean Data\n",
    "\n",
    "- Import data from Excel\n",
    "- Remove whitespace from column names\n",
    "- Fill missing values with column means\n",
    "\n",
    "### 2. Define Variables\n",
    "\n",
    "- **X**: Feature matrix containing T1, M1, C1\n",
    "- **Y**: Target variable (`Level`)\n",
    "- **T**: Binary treatment variable (1 = Large Screen, 0 = Small Screen)\n",
    "\n",
    "### 3. Standardization (optional)\n",
    "\n",
    "- Log transform (commented out in this version)\n",
    "- Standardize values using `StandardScaler`\n",
    "\n",
    "### 4. Visualization\n",
    "\n",
    "- Plot histogram distributions of each feature\n",
    "- Display feature correlation matrix using seaborn heatmap\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Output\n",
    "\n",
    "- 📊 Feature summary statistics (before and after processing)\n",
    "- 🧱 Feature distributions (histograms)\n",
    "- 🔗 Feature correlation heatmap\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Next Step\n",
    "\n",
    "After this preprocessing, the prepared variables `X`, `Y`, and `T` are ready to be passed into the `CausalForestDML` model for estimating individual treatment effects.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note**: This script uses `econml`, `scikit-learn`, `seaborn`, and `matplotlib` for preprocessing and visualization. Be sure to install the required packages before running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad8a07-05b2-490d-ab40-adf6678dff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load Excel data\n",
    "file_path = \"RSR Largescreen_Smallscreen customer.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 2. Check and handle missing values\n",
    "if data.isnull().sum().sum() > 0:\n",
    "    print(\"Missing values detected. Handling missing values...\")\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Strip any extra whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# 3. Display original feature statistics\n",
    "print(\"\\n--- Original Feature Statistics ---\")\n",
    "print(data[['Feature T1', 'Feature M1', 'Feature C1']].describe())\n",
    "\n",
    "# (Optional) Apply log transformation to reduce skewness\n",
    "# data['Feature T1'] = np.log1p(data['Feature T1'])\n",
    "# data['Feature M1'] = np.log1p(data['Feature M1'])\n",
    "# data['Feature C1'] = np.log1p(data['Feature C1'])\n",
    "\n",
    "# Standardize all features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data[['Feature T1', 'Feature M1', 'Feature C1']])\n",
    "\n",
    "# Prepare features and labels\n",
    "X = data[['Feature T1', 'Feature M1', 'Feature C1']]\n",
    "Y = data['Level'].values\n",
    "T = data['Screen'].apply(lambda x: 1 if x == 'L' else 0).values  # L = 1 (Large screen), others = 0\n",
    "\n",
    "# Ensure treatment variable is integer\n",
    "T = T.astype(int)\n",
    "\n",
    "# Display statistics after processing\n",
    "print(\"\\n--- Processed Feature Statistics ---\")\n",
    "print(pd.DataFrame(X, columns=['Feature T1', 'Feature M1', 'Feature C1']).describe())\n",
    "\n",
    "# Plot histograms of processed features\n",
    "pd.DataFrame(X, columns=['Feature T1', 'Feature M1', 'Feature C1']).hist(bins=30, figsize=(10, 6))\n",
    "plt.suptitle(\"Processed Feature Distributions\")\n",
    "plt.show()\n",
    "\n",
    "# Check feature correlation\n",
    "print(\"\\n--- Feature Correlation Matrix ---\")\n",
    "correlation_matrix = data[['Feature T1', 'Feature M1', 'Feature C1']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f95b6-7b84-4eaa-9861-3d4abde4269b",
   "metadata": {},
   "source": [
    "# 🧪 Stratified Train-Test Split for Causal Inference\n",
    "\n",
    "This script performs a **stratified sampling split** of the dataset for causal inference, ensuring that the training and test sets maintain the same distribution across treatment groups and outcome levels.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Purpose\n",
    "\n",
    "When estimating **Individual Treatment Effects (ITE)** using methods like Causal Forests, it's important that:\n",
    "\n",
    "- Treatment assignment (`Screen`)\n",
    "- Outcome levels (`Level`)\n",
    "\n",
    "...are evenly distributed between training and testing datasets. This ensures that the model learns from a balanced and representative dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Steps\n",
    "\n",
    "### 1. Clean and Format Target Variable\n",
    "\n",
    "- Convert `Level` to integers\n",
    "- Remove rows with unexpected levels (only keep 1, 2, or 3)\n",
    "\n",
    "### 2. Create Stratification Variable\n",
    "\n",
    "- Combine `Screen` and `Level` into a composite variable:  \n",
    "  e.g., `L_1`, `S_3`, etc.\n",
    "- Used for stratified sampling\n",
    "\n",
    "### 3. Perform Stratified Split\n",
    "\n",
    "- Use `train_test_split` with the composite `Stratify` column\n",
    "- Split into training (80%) and testing (20%) sets\n",
    "\n",
    "### 4. Validate the Split\n",
    "\n",
    "- Print shape of split datasets\n",
    "- Count stratification label distribution in each subset\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Output\n",
    "\n",
    "- Balanced train/test splits across treatment & outcome strata\n",
    "- Console output showing distribution consistency\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why This Matters\n",
    "\n",
    "Stratified sampling is crucial in **causal machine learning** to ensure valid and unbiased comparisons between groups. If the treatment or outcome distribution is skewed post-split, it may introduce confounding or reduce generalizability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe02f2-5f0b-4a6b-85a6-58b2470123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Round and convert 'Level' to integer (e.g., in case of floating point noise)\n",
    "data['Level'] = data['Level'].round().astype(int)\n",
    "\n",
    "# Filter out invalid values (keep only Levels 1, 2, 3)\n",
    "data = data[data['Level'].isin([1, 2, 3])]\n",
    "\n",
    "# Create a new stratification variable combining treatment and outcome\n",
    "data['Stratify'] = data['Screen'] + \"_\" + data['Level'].astype(str)\n",
    "\n",
    "stratify_var = data['Stratify']\n",
    "\n",
    "# Print unique values and counts for stratification\n",
    "print(\"Unique values in Stratify column:\", data['Stratify'].unique())\n",
    "print(\"Stratify value counts:\\n\", data['Stratify'].value_counts())\n",
    "\n",
    "# Perform stratified train-test split to preserve treatment and outcome balance\n",
    "X_train, X_test, Y_train, Y_test, T_train, T_test, stratify_train, stratify_test = train_test_split(\n",
    "    X, Y, T, stratify_var,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=stratify_var\n",
    ")\n",
    "\n",
    "# Check the shape of the split datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"T_train shape:\", T_train.shape)\n",
    "print(\"Stratify_train shape:\", len(stratify_train))\n",
    "\n",
    "# Display stratification counts for training and testing sets\n",
    "train_counts = pd.Series(stratify_train).value_counts()\n",
    "test_counts = pd.Series(stratify_test).value_counts()\n",
    "\n",
    "print(\"\\nTraining Set Stratify Counts:\\n\", train_counts)\n",
    "print(\"\\nTest Set Stratify Counts:\\n\", test_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca9f09-3089-4194-b9b2-7b08cfd33d80",
   "metadata": {},
   "source": [
    "# 🔍 One-Hot Encoding for Stratified Covariates\n",
    "\n",
    "This script applies one-hot encoding to the stratification variable for use in advanced causal inference models (e.g., `CausalForestDML` from the `econml` package).\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Purpose\n",
    "\n",
    "Stratification variables often need to be converted into numeric form when used as control variables or covariates. Here, **one-hot encoding** ensures categorical stratification levels (like `\"L_1\"`, `\"S_3\"`) are usable in models that require numerical input.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Steps\n",
    "\n",
    "### 1. One-Hot Encoding\n",
    "\n",
    "- Encode the composite stratification variable (`Screen_Level`)\n",
    "- Use `OneHotEncoder` from `sklearn`\n",
    "- `sparse_output=False` ensures dense matrix for model compatibility\n",
    "\n",
    "### 2. Array Shape Validation\n",
    "\n",
    "- Print shapes of encoded arrays\n",
    "- Ensure number of samples match across:\n",
    "  - `X_train` (features)\n",
    "  - `Y_train` (target)\n",
    "  - `stratify_train_encoded` (controls)\n",
    "\n",
    "### 3. Type Checks (Debug)\n",
    "\n",
    "- Confirm data structure integrity\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Output\n",
    "\n",
    "- Encoded stratification matrices (train & test)\n",
    "- Console output with shape info\n",
    "- Assertion to verify row alignment\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Why It Matters\n",
    "\n",
    "Causal inference models like **Double Machine Learning (DML)** require precise alignment of:\n",
    "\n",
    "- Treatments\n",
    "- Outcomes\n",
    "- Covariates (controls)\n",
    "\n",
    "Improper encoding can lead to **dimension mismatches** or **biased estimates**. One-hot encoding helps safely incorporate stratification into the causal framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905751a-91ed-4740-af5d-1cb39fbf4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# One-hot encode the stratification variable\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Reshape the Series to 2D array for encoder input\n",
    "stratify_train_encoded = encoder.fit_transform(stratify_train.values.reshape(-1, 1))\n",
    "stratify_test_encoded = encoder.transform(stratify_test.values.reshape(-1, 1))\n",
    "\n",
    "# Print encoded shape for verification\n",
    "print(\"stratify_train_encoded shape:\", stratify_train_encoded.shape)\n",
    "print(\"stratify_test_encoded shape:\", stratify_test_encoded.shape)\n",
    "\n",
    "# Print original training data shape\n",
    "print(\"X_train shape:\", X_train.shape)  # (n_samples, n_features)\n",
    "print(\"Y_train shape:\", Y_train.shape)  # (n_samples,)\n",
    "\n",
    "# Sanity check: number of samples must match across arrays\n",
    "assert X_train.shape[0] == Y_train.shape[0] == stratify_train_encoded.shape[0], \"Inconsistent sample sizes!\"\n",
    "\n",
    "# Print object types for debugging\n",
    "print(type(stratify_train))\n",
    "print(type(stratify_train.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e61930-e35d-4d7d-9568-efbe2ef78d18",
   "metadata": {},
   "source": [
    "# 🌲 Causal Forest with Hyperparameter Optimization\n",
    "\n",
    "This script builds a **Causal Forest model** using `econml`'s `CausalForestDML`, with a full pipeline including **data cleaning**, **hyperparameter tuning**, and **treatment effect estimation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Components\n",
    "\n",
    "- **Outcome Model (`model_y`)**: `RandomForestRegressor`, optimized using `GridSearchCV` with `r2` as the scoring metric.\n",
    "- **Treatment Model (`model_t`)**: `RandomForestClassifier`, optimized with `f1_weighted` scoring.\n",
    "- **Causal Estimation**: Estimates **heterogeneous treatment effects (HTE)** and **average treatment effect (ATE)** using the Causal Forest algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Workflow\n",
    "\n",
    "### Step 1: Preprocessing\n",
    "- Split dataset into training and test sets.\n",
    "- Replace missing or invalid values using `np.nan_to_num`.\n",
    "\n",
    "### Step 2: Model Optimization\n",
    "- Use `GridSearchCV` to tune hyperparameters for both the outcome and treatment models.\n",
    "- Models are selected based on cross-validated performance.\n",
    "\n",
    "### Step 3: Train Causal Forest\n",
    "- Initialize `CausalForestDML` with optimized sub-models.\n",
    "- Fit the model using the training data.\n",
    "\n",
    "### Step 4: Evaluate\n",
    "- Predict **individual treatment effects** using `.effect()`.\n",
    "- Estimate **Average Treatment Effect (ATE)** using `.ate()`.\n",
    "- Compute **confidence intervals** using `.ate_interval()`.\n",
    "\n",
    "### Step 5: Visualization\n",
    "- Histogram of estimated treatment effects.\n",
    "- Summary statistics and cross-validated scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Output\n",
    "\n",
    "- Optimized parameters for `model_y` and `model_t`\n",
    "- Cross-validated F1 scores for treatment model\n",
    "- ATE estimate and confidence interval\n",
    "- Treatment effect distribution plot\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Notes\n",
    "\n",
    "- `discrete_treatment=True` is required since treatment is binary.\n",
    "- A high number of trees (`n_estimators=1000`) ensures more stable effect estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ae3be-fd51-4c46-af97-3cfd9751834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Split data into training and test sets\n",
    "X_train, X_test, T_train, T_test, Y_train, Y_test = train_test_split(X, T, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Clean the data (handle NaNs and negative values if needed)\n",
    "Y_train = np.nan_to_num(Y_train)\n",
    "T_train = np.nan_to_num(T_train)\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# Step 3: Define hyperparameter grid for model tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Step 4: Grid search for outcome model (Y)\n",
    "grid_search_y = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring='r2')\n",
    "grid_search_y.fit(X_train, Y_train)\n",
    "\n",
    "# Step 5: Grid search for treatment model (T)\n",
    "grid_search_t = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1_weighted')\n",
    "grid_search_t.fit(X_train, T_train)\n",
    "\n",
    "# Step 6: Use best estimators\n",
    "optimized_model_y = grid_search_y.best_estimator_\n",
    "optimized_model_t = grid_search_t.best_estimator_\n",
    "\n",
    "print(\"Optimized model_y parameters:\", grid_search_y.best_params_)\n",
    "print(\"Optimized model_t parameters:\", grid_search_t.best_params_)\n",
    "\n",
    "# Step 7: Initialize causal forest using optimized models\n",
    "causal_forest = CausalForestDML(\n",
    "    model_y=optimized_model_y,\n",
    "    model_t=optimized_model_t,\n",
    "    discrete_treatment=True,\n",
    "    n_estimators=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 8: Evaluate treatment model using cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(causal_forest.model_t, X_train, T_train, cv=kf, scoring='f1_weighted')\n",
    "print(\"Cross-Validation F1 Scores for Treatment Model:\", cross_val_scores)\n",
    "print(\"Mean F1 Score:\", np.mean(cross_val_scores))\n",
    "\n",
    "# Step 9: Fit causal forest\n",
    "try:\n",
    "    causal_forest.fit(Y_train, T_train, X=X_train)\n",
    "    print(\"Causal forest model fitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during fitting: {e}\")\n",
    "\n",
    "# Step 10: Estimate treatment effects\n",
    "try:\n",
    "    treatment_effects = causal_forest.effect(X_test)\n",
    "    print(\"Treatment Effects (first 10):\", treatment_effects[:10])\n",
    "except Exception as e:\n",
    "    print(f\"Error predicting treatment effects: {e}\")\n",
    "\n",
    "# Step 11: Estimate average treatment effect (ATE)\n",
    "try:\n",
    "    ate = causal_forest.ate(X_test)\n",
    "    print(\"Average Treatment Effect (ATE):\", ate)\n",
    "except Exception as e:\n",
    "    print(f\"Error computing ATE: {e}\")\n",
    "\n",
    "# Step 12: Plot treatment effect distribution\n",
    "try:\n",
    "    plt.hist(treatment_effects, bins=30, edgecolor='k')\n",
    "    plt.title(\"Distribution of Estimated Treatment Effects\")\n",
    "    plt.xlabel(\"Treatment Effect\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error in plotting treatment effect distribution: {e}\")\n",
    "\n",
    "# Step 13: Compute confidence interval for ATE\n",
    "try:\n",
    "    lb, ub = causal_forest.ate_interval(X_test)\n",
    "    print(f\"ATE Confidence Interval: [{lb}, {ub}]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error computing ATE confidence interval: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379725b5-ea92-4454-bc23-32801f01fad7",
   "metadata": {},
   "source": [
    "# 🎯 Individual Treatment Effect (ITE) Analysis with Causal Forest\n",
    "\n",
    "This workflow focuses on estimating and analyzing **individual treatment effects (ITE)** using a fitted `CausalForestDML` model, along with **grouped comparisons**, **feature importance extraction**, and **visualizations**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Core Workflow\n",
    "\n",
    "### Step 1: Estimate ITE\n",
    "- Use `.effect(X)` to compute the personalized treatment effect for each observation.\n",
    "\n",
    "### Step 2: Grouping by Device Type and Engagement Level\n",
    "- Create a summary DataFrame with:\n",
    "  - `ITE`: predicted individual treatment effect\n",
    "  - `DeviceType`: treatment group (e.g., large screen vs. small screen)\n",
    "  - `EngagementLevel`: behavioral engagement (1, 2, 3)\n",
    "\n",
    "- Compute average treatment effect within each subgroup:\n",
    "```python\n",
    "grouped_df = ite_df.groupby(['DeviceType', 'EngagementLevel'])['ITE'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb375d-6760-4249-a9de-5857f709c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Predict Individual Treatment Effects (ITE)\n",
    "ite = causal_forest.effect(X)\n",
    "\n",
    "# Extract device type column (assumed to represent treatment assignment)\n",
    "device_type = data['Screen']  # e.g., 'L' for large, 'S' for small\n",
    "\n",
    "# Ensure engagement level column is valid\n",
    "data['Level'] = data['Level'].round().astype(int)  # Convert to integer\n",
    "data = data[data['Level'].isin([1, 2, 3])]         # Keep only valid levels\n",
    "\n",
    "# Validate data cleaning\n",
    "if set(data['Level'].unique()) <= {1, 2, 3}:\n",
    "    print(\"Engagement level cleaned successfully: only 1, 2, 3 present.\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid values remain in Level column.\")\n",
    "\n",
    "# Create DataFrame for ITE analysis\n",
    "ite_df = pd.DataFrame({\n",
    "    'ITE': ite,\n",
    "    'DeviceType': data['Screen'],\n",
    "    'EngagementLevel': data['Level']\n",
    "})\n",
    "\n",
    "# Compute group-wise average ITE\n",
    "group_effects = ite_df.groupby(['DeviceType', 'EngagementLevel'])['ITE'].mean().reset_index()\n",
    "print(\"Grouped Effects:\")\n",
    "print(group_effects)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = causal_forest.feature_importances()\n",
    "behavior_variables = ['T1', 'M1', 'C1']  # Assumed behavioral features\n",
    "\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': behavior_variables,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "print(\"Feature Importance:\")\n",
    "print(importances_df)\n",
    "\n",
    "# Visualize ITE distribution by device type and engagement level\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=ite_df, x='DeviceType', y='ITE', hue='EngagementLevel')\n",
    "plt.title('ITE Distribution by Device Type and Engagement Level')\n",
    "plt.xlabel('Device Type')\n",
    "plt.ylabel('Individual Treatment Effect (ITE)')\n",
    "plt.legend(title='Engagement Level')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance of behavioral variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=importances_df, x='Feature', y='Importance')\n",
    "plt.title('Feature Importance of Behavioral Variables')\n",
    "plt.xlabel('Behavioral Variable')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Fit causal forest using built-in cross-validation\n",
    "causal_forest = CausalForestDML(\n",
    "    model_y=RandomForestRegressor(),\n",
    "    model_t=RandomForestClassifier(),\n",
    "    discrete_treatment=True,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "causal_forest.fit(Y_train, T_train, X=X_train)\n",
    "print(\"Causal forest fitted successfully with cross-validation!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
